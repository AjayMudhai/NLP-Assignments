{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ajaymudhai/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ajaymudhai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import nltk.data\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "url = \"http://www.gutenberg.org/cache/epub/1661/pg1661.txt\"\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.1 and Q.2 Downloading dataset and removing unnecessary symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = request.urlopen(url)\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters, PunktLanguageVars\n",
    "raw = response.read().decode('utf8')\n",
    "text=raw.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ').replace(',',' ').replace(\"''\",\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.2 Senetence Tokenizing and dividing sentences into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6960\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "l=len(sent_tokenize_list)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding <s> and </s> to mark start and end of sentence\"\"\"\n",
    "for i in range(l):\n",
    "    sent_tokenize_list[i]=\"<s>\"+sent_tokenize_list[i]+\"</s>\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset=sent_tokenize_list[:5568]\n",
    "test_dataset=sent_tokenize_list[5568:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unigram, bigram, trigrams and quadgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "token = nltk.word_tokenize(text)\n",
    "token = nltk.word_tokenize(text)\n",
    "\n",
    "unigrams = ngrams(text.split(), 1)\n",
    "\n",
    "bigrams = ngrams(text.split(), 2)\n",
    "trigrams = ngrams(text.split(), 3)\n",
    "quadgrams = ngrams(text.split(), 4)\n",
    "\n",
    "uf=dict()\n",
    "vocab=0\n",
    "for x in unigrams:\n",
    "    if x not in uf:\n",
    "        uf[x]=1\n",
    "        vocab+=1\n",
    "    else:\n",
    "        uf[x]+=1\n",
    "        vocab+=1\n",
    "\n",
    "bf=dict()\n",
    "bfl=0\n",
    "for x in bigrams:\n",
    "    if x not in bf:\n",
    "        bf[x]=1\n",
    "        bfl+=1\n",
    "    else:\n",
    "        bf[x]+=1\n",
    "        bfl+=1\n",
    "\n",
    "tf=dict()\n",
    "for x in trigrams:\n",
    "    if x not in tf:\n",
    "        tf[x]=1\n",
    "    else:\n",
    "        tf[x]+=1\n",
    "\n",
    "qf=dict()\n",
    "for x in quadgrams:\n",
    "    if x not in qf:\n",
    "        qf[x]=1\n",
    "    else:\n",
    "        qf[x]+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for Unigram(probabilities of individual words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project 8.238589553468446e-06\n",
      "Gutenberg 0.00023891909705058494\n",
      "'s 0.0029988465974625142\n",
      "The 0.0028505519855000824\n",
      "Adventures 2.4715768660405338e-05\n"
     ]
    }
   ],
   "source": [
    "freq_1gram = nltk.FreqDist(token)\n",
    "\n",
    "len_corpus = len(token)\n",
    "def unigram_prob(word):\n",
    "\n",
    "    return freq_1gram[ word] / len_corpus\n",
    "nl=[]\n",
    "for x in token:\n",
    "    nl.append(unigram_prob(x))\n",
    "for i in range(5):\n",
    "    print(token[i],nl[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffProject', \"Gutenberg's\") 1.0\n",
      "(\"Gutenberg's\", 'The') 1.0\n",
      "('The', 'Adventures') 0.01098901098901099\n",
      "('Adventures', 'of') 1.0\n",
      "('of', 'Sherlock') 0.0018355359765051395\n"
     ]
    }
   ],
   "source": [
    "bmle={}\n",
    "for x in bf:\n",
    "    bmle[x]=bf[x]/uf[(x[0],)]\n",
    "bl=list(bmle)\n",
    "\n",
    "for i in range(5):\n",
    "    print(bl[i],bmle[bl[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for trigram \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffProject', \"Gutenberg's\", 'The') 1.0\n",
      "(\"Gutenberg's\", 'The', 'Adventures') 1.0\n",
      "('The', 'Adventures', 'of') 1.0\n",
      "('Adventures', 'of', 'Sherlock') 1.0\n",
      "('of', 'Sherlock', 'Holmes') 0.6\n"
     ]
    }
   ],
   "source": [
    "tmle={}\n",
    "for x in tf:\n",
    "    tmle[x]=tf[x]/bf[(x[0],x[1])]\n",
    "tl=list(tmle)\n",
    "\n",
    "for i in range(5):\n",
    "    print(tl[i],tmle[tl[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for quadgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffProject', \"Gutenberg's\", 'The', 'Adventures') 1.0\n",
      "(\"Gutenberg's\", 'The', 'Adventures', 'of') 1.0\n",
      "('The', 'Adventures', 'of', 'Sherlock') 1.0\n",
      "('Adventures', 'of', 'Sherlock', 'Holmes') 1.0\n",
      "('of', 'Sherlock', 'Holmes', 'by') 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "qmle={}\n",
    "for x in qf:\n",
    "    qmle[x]=qf[x]/tf[(x[0],x[1],x[2])]\n",
    "ql=list(qmle)\n",
    "\n",
    "for i in range(5):\n",
    "    print(ql[i],qmle[ql[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.5 Add-1 Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffProject', \"Gutenberg's\")\n",
      "Bigram MLE before smoothing : 1.000000\n",
      "After addone smoothing : 0.000018\n",
      "(\"Gutenberg's\", 'The')\n",
      "Bigram MLE before smoothing : 1.000000\n",
      "After addone smoothing : 0.000018\n",
      "('The', 'Adventures')\n",
      "Bigram MLE before smoothing : 3.000000\n",
      "After addone smoothing : 0.000037\n",
      "('Adventures', 'of')\n",
      "Bigram MLE before smoothing : 3.000000\n",
      "After addone smoothing : 0.000037\n",
      "('of', 'Sherlock')\n",
      "Bigram MLE before smoothing : 5.000000\n",
      "After addone smoothing : 0.000054\n"
     ]
    }
   ],
   "source": [
    "addone={}\n",
    "for x in bf:\n",
    "    addone[x]=(bf[x]+1)/(uf[(x[0],)]+vocab)\n",
    "    \n",
    "al=list(addone)\n",
    "bfl=list(bf)\n",
    "for i in range(5):\n",
    "    print(bfl[i])\n",
    "    print(\"Bigram MLE before smoothing : %f\"%bf[bfl[i]])\n",
    "    print(\"After addone smoothing : %f\"%addone[al[i]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant change can be seen after Add-1 Smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q.6 Good-turing smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4375450862882193 -0.4375450862882193\n",
      "1 0.2527584020291693 0.7472415979708307\n",
      "2 1.0727546412443552 0.9272453587556448\n",
      "3 1.8858746492048644 1.1141253507951356\n",
      "4 3.0307539682539684 0.9692460317460316\n",
      "5 4.016366612111293 0.9836333878887071\n",
      "6 4.58679706601467 1.41320293398533\n",
      "7 6.17910447761194 0.8208955223880601\n",
      "8 6.434782608695652 1.5652173913043477\n",
      "9 9.324324324324325 -0.32432432432432456\n"
     ]
    }
   ],
   "source": [
    "fd={}\n",
    "for x in bf.values():\n",
    "    if x not in fd:\n",
    "        fd[x]=1\n",
    "    else:\n",
    "        fd[x]+=1\n",
    "fd[0]=bfl\n",
    "\n",
    "gts={}\n",
    "for i in range(10,-1,-1):\n",
    "    gts[i]=((i + 1) * fd[i + 1]) / fd[i]\n",
    "\n",
    "for i in range(10):\n",
    "    print(i,gts[i],i-gts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7778938164221545"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D=0\n",
    "for i in range(10):\n",
    "    D=D+i-gts[i]\n",
    "D=D/10\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d= 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
